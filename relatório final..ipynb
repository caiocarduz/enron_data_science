{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enron Submission Free-Response Questions\n",
    "\n",
    "\n",
    "A critical part of machine learning is making sense of your analysis process and communicating it to others. The questions below will help us understand your decision-making process and allow us to give feedback on your project. Please answer each question; your answers should be about 1-2 paragraphs per question. If you find yourself writing much more than that, take a step back and see if you can simplify your response!\n",
    "\n",
    "\n",
    "When your evaluator looks at your responses, he or she will use a specific list of rubric items to assess your answers. Here is the link to that rubric: [Link] Each question has one or more specific rubric items associated with it, so before you submit an answer, take a look at that part of the rubric. If your response does not meet expectations for all rubric points, you will be asked to revise and resubmit your project. Make sure that your responses are detailed enough that the evaluator will be able to understand the steps you took and your thought processes as you went through the data analysis.\n",
    "\n",
    "\n",
    "Once you’ve submitted your responses, your coach will take a look and may ask a few more focused follow-up questions on one or more of your answers.  \n",
    "\n",
    "\n",
    "We can’t wait to see what you’ve put together for this project!\n",
    "\n",
    "\n",
    "\n",
    "1-Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "## DataSet\n",
    "O dataset consiste em informações financeiras e de comunicação dos empregados da extinta corporação Enron, envolvida em um escândalo de fraudes contábeis que a levaram a falência e a intensa investigação por autoridades governamentais. \n",
    "De forma sucinta, a tentativa deste projeto é através destas informações identificar funcionários que participaram ativamente do esquema de fraude.\n",
    "As técnicas a serem utilizadas compreendem a exploração do dado, redução de dimensionalide, aplicação de um dos algoritmos de machine learnining e a validação dos resultados.\n",
    "\n",
    "## Exploração dos dados\n",
    "\n",
    "No total há 146 amostras e 20 \"features\". Muitos amostras apresentam features com \"null\" o que requeriu um preenchimento dessas ou por 0, ou alguma característica de tendência central, como média e mediana. Abaixo é possível ver todas as features e a quantidade de não null nas amostras.\n",
    "\n",
    "poi                          146 non-null bool\n",
    "\n",
    "bonus                        82 non-null float64\n",
    "\n",
    "deferred_income              49 non-null float64\n",
    "\n",
    "deferral_payments            39 non-null float64\n",
    "\n",
    "loan_advances                4 non-null float64\n",
    "\n",
    "other                        93 non-null float64\n",
    "\n",
    "expenses                     95 non-null float64\n",
    "\n",
    "director_fees                17 non-null float64\n",
    "\n",
    "total_payments               125 non-null float64\n",
    "\n",
    "exercised_stock_options      102 non-null float64\n",
    "\n",
    "restricted_stock             110 non-null float64\n",
    "\n",
    "restricted_stock_deferred    18 non-null float64\n",
    "\n",
    "total_stock_value            126 non-null float64\n",
    "\n",
    "to_messages                  86 non-null float64\n",
    "\n",
    "from_messages                86 non-null float64\n",
    "\n",
    "from_this_person_to_poi      86 non-null float64\n",
    "\n",
    "from_poi_to_this_person      86 non-null float64\n",
    "\n",
    "long_term_incentive          66 non-null float64\n",
    "\n",
    "salary                       95 non-null float64\n",
    "\n",
    "poi_email_ratio              86 non-null float64\n",
    "\n",
    "\n",
    "De forma a garantir a melhor qualidade das amostras tomou-se a estratégia de preencher as features financeiras sem valor com 0 pois se assumiu como premissa que valores nulos no dataset significam o não pagamento de determinado benefício ao funcionário.\n",
    "Abaixo é possível ver as caracteríticas das amostras posteriomente a substituição por 0.\n",
    "\n",
    "poi                          146 non-null bool\n",
    "\n",
    "bonus                        146 non-null float64\n",
    "\n",
    "deferred_income              146 non-null float64\n",
    "\n",
    "deferral_payments            146 non-null float64\n",
    "\n",
    "loan_advances                146 non-null float64\n",
    "\n",
    "other                        146 non-null float64\n",
    "\n",
    "expenses                     146 non-null float64\n",
    "\n",
    "director_fees                146 non-null float64\n",
    "\n",
    "total_payments               146 non-null float64\n",
    "\n",
    "exercised_stock_options      146 non-null float64\n",
    "\n",
    "restricted_stock             146 non-null float64\n",
    "\n",
    "restricted_stock_deferred    146 non-null float64\n",
    "\n",
    "total_stock_value            146 non-null float64\n",
    "\n",
    "to_messages                  86 non-null float64\n",
    "\n",
    "from_messages                86 non-null float64\n",
    "\n",
    "from_this_person_to_poi      86 non-null float64\n",
    "\n",
    "from_poi_to_this_person      86 non-null float64\n",
    "\n",
    "long_term_incentive          66 non-null float64\n",
    "\n",
    "salary                       95 non-null float64\n",
    "\n",
    "poi_email_ratio              146 non-null float64\n",
    "\n",
    "\n",
    "Com relação as variáveis de comunicação, optou-se pela substituição dos valores nulos pela média de cada uma das classes de POI. Neste caso, a falta de informação não a classifica necessariamente com a não interação entre os agentes e, portanto, o uso da mediana poderia ser justificado. Abaixo é mostrado as carateristicas gerais do dataset tratado.\n",
    "\n",
    "poi                          146 non-null bool\n",
    "\n",
    "bonus                        146 non-null float64\n",
    "\n",
    "deferred_income              146 non-null float64\n",
    "\n",
    "deferral_payments            146 non-null float64\n",
    "\n",
    "loan_advances                146 non-null float64\n",
    "\n",
    "other                        146 non-null float64\n",
    "\n",
    "expenses                     146 non-null float64\n",
    "\n",
    "director_fees                146 non-null float64\n",
    "\n",
    "total_payments               146 non-null float64\n",
    "\n",
    "exercised_stock_options      146 non-null float64\n",
    "\n",
    "restricted_stock             146 non-null float64\n",
    "\n",
    "restricted_stock_deferred    146 non-null float64\n",
    "\n",
    "total_stock_value            146 non-null float64\n",
    "\n",
    "to_messages                  146 non-null float64\n",
    "\n",
    "from_messages                146 non-null float64\n",
    "\n",
    "from_this_person_to_poi      146 non-null float64\n",
    "\n",
    "from_poi_to_this_person      146 non-null float64\n",
    "\n",
    "long_term_incentive          146 non-null float64\n",
    "\n",
    "salary                       146 non-null float64\n",
    "\n",
    "poi_to_email                 146 non-null float64\n",
    "\n",
    "poi_from_email               146 non-null float64\n",
    "\n",
    "\n",
    "Apenas um outlier foi encontrado o qual corresponde a amostra TOTAL. Esta soma todos os pagamentos efetuados pela empresa e por não corresponder a uma característica individual foi removida do datset.\n",
    "\n",
    "2-What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n",
    "\n",
    "\n",
    "## Seleção de features\n",
    "\n",
    "Para selecionar as principais features foram utilizados duas técnicas: KbestSelector, e DecisionTreeClassifier.\n",
    "\n",
    "O primeiro algoritmo classifica as features através de \"scores\" no qual aquelas que apresentarem valores maiores são as consideradas de maior relevância. De forma geral, os maiores \"scores\" foram atribuídos as variáveis financeiras e  classificação geral pode ser observado abaixo:\n",
    "\n",
    "[('exercised_stock_options', 24.95769208640269),\n",
    "\n",
    " ('total_stock_value', 24.554778199460614),\n",
    " \n",
    " ('bonus', 20.792252047181535),\n",
    " \n",
    " ('salary', 18.33836775749037),\n",
    " \n",
    " ('deferred_income', 11.466222228027991),\n",
    " \n",
    " ('long_term_incentive', 10.004464322324353),\n",
    " \n",
    " ('restricted_stock', 9.725202204751493),\n",
    " \n",
    " ('total_payments', 8.808881189792034),\n",
    " \n",
    " ('loan_advances', 7.184055658288725),\n",
    " \n",
    " ('expenses', 6.17412430056438),\n",
    " \n",
    " ('poi_from_email', 5.112000105553864),\n",
    " \n",
    " ('other', 4.369789663273286),\n",
    " \n",
    " ('from_poi_to_this_person', 2.962567365593336),\n",
    " \n",
    " ('poi_to_email', 2.10517496370243),\n",
    " \n",
    " ('director_fees', 1.9498838014353457),\n",
    " \n",
    " ('from_this_person_to_poi', 1.3200142577726062),\n",
    " \n",
    " ('from_messages', 0.6033909778782933),\n",
    " \n",
    " ('to_messages', 0.33780816490544013),\n",
    " \n",
    " ('deferral_payments', 0.2246112747360099),\n",
    " \n",
    " ('restricted_stock_deferred', 0.06549965290994214)]\n",
    " \n",
    " O outro algoritmo utilizado, diferentemente, não apresentou a mesma ordem de importância. A feature mais importante no caso da decision tree é bonus enquanto kbest é exercised stock option. a Abaixo é possível ver a relevância das features que tiverem \"score\" maiores que zero.\n",
    " \n",
    "other                        0.029966\n",
    "\n",
    "salary                       0.031684\n",
    "\n",
    "deferred_income              0.042245\n",
    "\n",
    "from_this_person_to_poi      0.049580\n",
    "\n",
    "from_messages                0.055447\n",
    "\n",
    "exercised_stock_options      0.058568\n",
    "\n",
    "expenses                     0.062307\n",
    "\n",
    "poi_to_email                 0.096561\n",
    "\n",
    "long_term_incentive          0.108631\n",
    "\n",
    "restricted_stock             0.120300\n",
    "\n",
    "total_payments               0.164592\n",
    "\n",
    "bonus                        0.180118\n",
    "\n",
    "Em termos de resultado, a segunda opção apresentou melhor resultado pois o classificador DecisionTree foi aquele que apresentou melhor precision e recall como veremos mais a frente. Posssivelmente, Kbest seria opção otimizada para algoritmos lineares como regressão logística.  \n",
    "\n",
    "## Scaling\n",
    "\n",
    "O \"scaling\" foi utilizado com os classificadores lineares como regressão logística e SVC porém quando comparados aos classificadores (decisionTree, Kneighbors e random forest) apresentaram resultado de f1 muito abaixo daqueles e portanto, não foram utilizados.\n",
    "\n",
    "## Criação de features\n",
    "\n",
    "foram criadas duas features, a saber: Poi_to_email e poi_from_email.\n",
    "A primeira é a razão entre emails enviados para pessoas de interesse e o total de mensagens recebidas. O racional desta feature e estabelecer com que frequencia, em relação ao total de mensagens enviadas, o agente se comumica com POIs o que poderia sugerir a sua qualificação como POIs ou não. \n",
    "No que tange a segunda feature, ela representa a razão entre emails recebidos de POIs e o total de emails recebidos. O racional é entender com que frequencia o agente recebe informações de POIs com relação a base total de contatos.\n",
    "As features foram criadas como funções.\n",
    "\n",
    "\n",
    "\n",
    "3-What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "\n",
    "O Algoritmo que apresentou melhor performance foi DecisionTree utilizando as features com relevância maior que 0. As features utilizadas, a saber:\n",
    "\n",
    "other                        0.029966\n",
    "\n",
    "salary                       0.031684\n",
    "\n",
    "deferred_income              0.042245\n",
    "\n",
    "from_this_person_to_poi      0.049580\n",
    "\n",
    "from_messages                0.055447\n",
    "\n",
    "exercised_stock_options      0.058568\n",
    "\n",
    "expenses                     0.062307\n",
    "\n",
    "poi_to_email                 0.096561\n",
    "\n",
    "long_term_incentive          0.108631\n",
    "\n",
    "restricted_stock             0.120300\n",
    "\n",
    "total_payments               0.164592\n",
    "\n",
    "bonus                        0.180118\n",
    "\n",
    "\n",
    "\n",
    "A estratégia de otimização seguiu a seguinte diretrizas:\n",
    "\n",
    "testou-se os classificadores Gaussian, Random Forest e Decision tree com todas as Kbest features e posteriormente, as K features mais importantes de espaçadas de 3 em 3. os resultados podem ser visualizados na tabela a seguir.\n",
    "\n",
    "|      f1/K     \t|    3    \t|    6    \t|    9    \t|    12   \t|    15   \t|    18   \t|    20   \t|\n",
    "|:-------------:\t|:-------:\t|:-------:\t|:-------:\t|:-------:\t|:-------:\t|:-------:\t|:-------:\t|\n",
    "|    Gaussian    \t| 0.31033 \t| 0.36164 \t| 0.34941 \t| 0.36164 \t| 0.32243 \t| 0.25281 \t| 0.27803 \t|\n",
    "| Decision Tree \t| 0.29911 \t| 0.23355 \t| 0.21416 \t| 0.17227 \t| 0.18903 \t| 0.19341 \t| 0.16002 \t|\n",
    "|  Random Fores \t| 0.30377 \t| 0.21981 \t| 0.20845 \t| 0.25076 \t| 0.24516 \t| 0.25772 \t| 0.29982 \t|\n",
    "\n",
    "\n",
    "Nesta estratégia, o melhor resultado de F1 foi de 0.36 obtido usando-se o classificador Gaussian com 12 features.\n",
    "\n",
    "Outra estratégia foi utilizar as features importance com valor maior que 0 para todos os classificadores. os resultados podem ser vistos na tabela a seguir:\n",
    "\n",
    "|               \t|      F1 \t|\n",
    "|--------------:\t|--------:\t|\n",
    "|       Gaussin \t| 0.28231 \t|\n",
    "| Decision Tree \t| 0.49317 \t|\n",
    "|  Random Fores \t|  0.2456 \t|\n",
    "\n",
    "\n",
    "Com esta abordagem. o maior F1 obtido foi de 0.49317 com o classificador Decision Tree. Em comparação a estratégia anterior, este último apresentou a melhor métrica de predição.\n",
    "\n",
    "Apesar do resultado superior a meta estipulada pelo curso, ainda tentou-se aplicar o uso da redução de dimensionalidade por meio da técnica de PCA e aquelas características mais importantes. Os resultados estão descritos abaixo:\n",
    "\n",
    "|               \t| 2       \t| 4       \t| 6       \t|\n",
    "|---------------\t|---------\t|---------\t|---------\t|\n",
    "| Gaussian      \t| 0.35517 \t| 0.35259 \t| 0.3194  \t|\n",
    "| Decision Tree \t| 0.41379 \t| 0.31172 \t| 03092   \t|\n",
    "| Random Forest \t| 0.22704 \t| 0.20999 \t| 0.22457 \t|\n",
    "\n",
    "Decision tree mostrou-se o melhor classificador porém não superou a estratégia anterior de somente as features mais importantes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Os resultados relacionados a melhor performance obtidos, a saber:\n",
    "\n",
    "DecisionTreeClassifier(class_weight={0: 6, 1: 40}, criterion='gini',\n",
    "\n",
    "            max_depth=3, max_features=None, max_leaf_nodes=None,\n",
    "            \n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            \n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            \n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=29,\n",
    "            \n",
    "            splitter='best')\n",
    "            \n",
    "        Accuracy: 0.79707       Precision: 0.36970      Recall: 0.74050 F1: 0.49317     F2: 0.61677\n",
    "        \n",
    "        Total predictions: 15000        True positives: 1481    False positives: 2525   False negatives:  519   True negatives: 10475\n",
    "\n",
    "\n",
    "\n",
    "A métrica de maior relevância para se caracterizar o melhor classificador é o F1 a qual é representada pela média harmônica da precisão e recall. Lembrar que o intuito do é minimizar o número de falsos positivos e falsos negativos de forma equilibrada e não um as custas do outro. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4-What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "\n",
    "\"tuning\" é essencialmente optimizar os parametros do classificador selecionado de forma que permita que o mesmo apresente a melhor performance possível em termos do trade-off Viés-Variância. Entenda-se por performance, maximizar acurácia, precision e recall do modelo.\n",
    "No projeto foi feito escolha manual por tentiva erro. Porém, há fluxos mais otimizados como o GridSearhCV no qul permite que diversos parametros sejam testados e escolhidos automaticamente de acordo com a melhor performance em termos de métricas.\n",
    "No caso em questão, os parâmetros do melhor classificador foi garantir um peso maior as amostras positivas devido a assimetria da distribuição dos eventos o que permitiu um melhor F1.\n",
    "\n",
    "DecisionTreeClassifier(class_weight={0: 6, 1: 40}, criterion='gini',\n",
    "\n",
    "            max_depth=3, max_features=None, max_leaf_nodes=None,\n",
    "            \n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            \n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            \n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=29,\n",
    "            \n",
    "            splitter='best')\n",
    "            \n",
    "        Accuracy: 0.79707       Precision:      Recall: 0.74050 F1: 0.49317     F2: 0.61677\n",
    "        \n",
    "        Total predictions: 15000        True positives: 1481    False positives: 2525   False negatives:  519   True negatives: 10475\n",
    "\n",
    "5-What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]\n",
    "\n",
    "Validação é medir a performance do classificador. Erro mais comum é treinar seu modelo e prevê-lo no mesmo conjunto de dados, ou seja, nao separá-lo em training e test data. Este tipo de erro pode levar as situação de \"overfitting\" que é a incapacidade do modelo generalizar o problema.\n",
    "Há diversas formas de validação, neste projeto foi utilizado o StratifiedShuffleSplit, que é essencialmente um  método de validação cruzada cuja característica é separar o dado em training/test, treinar/prever o resultado diversas vezes com pares training/test diferentes e, ao final, produzir a média dos resultados. Uma característica deste método é a sobreposição dos conjuntos selecionados para a validação cruzada o que não acontece com o método Kfolds. \n",
    "\n",
    "6-Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "Os resultados relacionados a performance obtidos, a saber:\n",
    "\n",
    "| acurácia \t| 0.79 \t|\n",
    "|----------\t|------\t|\n",
    "| precisão \t| 0.37 \t|\n",
    "| recall   \t| 0.74 \t|\n",
    "| F1       \t| 0.49 \t|\n",
    "| F2       \t| 0.62 \t|\n",
    "\n",
    "\n",
    "Precisão refere-se a razão de verdadeiro positivo e a soma dos falsos positivos e verdadeiros positivos. Ou seja, da a proporção, dentre aqueles que foram previstos como POIs, quantos efetivamente são POIs. De outra forma, quando o modelo preve que é POIs, ele está certo em 36,97% dos casos.\n",
    "\n",
    "Recall refere-se a razão veradeiro positivo e a soma do verdadeiro positivo e falso negativo. Ou seja, da a proporção de quantos POIs foram identificados corretamente. De outra forma, o modelo indentifica corretamente 74,05% dos POIs.\n",
    "\n",
    "Já o F1 é a média harmônica entre precision e recall. quanto mais próxima de 1 melhor a capcidade de previsão do modelo.\n",
    "\n",
    "## Referências \n",
    "\n",
    "https://olegleyz.github.io/enron_classifier.html\n",
    "\n",
    "https://github.com/nehal96/Machine-Learning-Enron-Fraud\n",
    "\n",
    "http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "https://pandas.pydata.org/\n",
    "\n",
    "https://www.tutorialspoint.com/python/python_dictionary.htm\n",
    "\n",
    "https://pt.wikipedia.org/wiki/Enron\n",
    "\n",
    "http://bailando.sims.berkeley.edu/enron_email.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memorial de cálculo.\n",
    "\n",
    "\n",
    "Configuração inicial, somente a feature Salary.\n",
    "\n",
    "GaussianNB(priors=None)\n",
    "\tAccuracy: 0.25560\tPrecision: 0.18481\tRecall: 0.79800\tF1: 0.30011\tF2: 0.47968\n",
    "\tTotal predictions: 10000\tTrue positives: 1596\tFalse positives: 7040\tFalse negatives:  404\tTrue negatives:  960\n",
    "\n",
    "\n",
    "Config inicial porem com rescolanamento de todas as features\n",
    "    GaussianNB(priors=None)\n",
    "\tAccuracy: 0.33800\tPrecision: 0.15456\tRecall: 0.88700\tF1: 0.26324\tF2: 0.45539\n",
    "\tTotal predictions: 15000\tTrue positives: 1774\tFalse positives: 9704\tFalse negatives:  226\tTrue negatives: 3296\n",
    "\n",
    "Config com 2 features shared poi e loan advance \n",
    "    GaussianNB(priors=None)\n",
    "\tAccuracy: 0.33585\tPrecision: 0.16857\tRecall: 0.84350\tF1: 0.28098\tF2: 0.46840\n",
    "\tTotal predictions: 13000\tTrue positives: 1687\tFalse positives: 8321\tFalse negatives:  313\tTrue negatives: 2679\n",
    "\n",
    "\n",
    "Randon forest com 10 estimadores\n",
    "\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\tAccuracy: 0.84940\tPrecision: 0.30408\tRecall: 0.10050\tF1: 0.15107\tF2: 0.11604\n",
    "\tTotal predictions: 15000\tTrue positives:  201\tFalse positives:  460\tFalse negatives: 1799\tTrue negatives: 12540\n",
    "    \n",
    "    Notar maior acuracia, no entanto menor precisão e recall levando F1 ser a metade do naive bayes. \n",
    "    Provavelmente o grande numero de features tenha sido o grande vilão. Re-escalar os dados pode ser uma alternativa, ou usar um numero menor de estimadores.\n",
    "    \n",
    "Rabdom forest com um estimador shared poi\n",
    "    andomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\tAccuracy: 0.83767\tPrecision: 0.26741\tRecall: 0.26500\tF1: 0.26620\tF2: 0.26548\n",
    "\tTotal predictions: 9000\tTrue positives:  265\tFalse positives:  726\tFalse negatives:  735\tTrue negatives: 7274\n",
    "    \n",
    "random forest shared poi and loan advance\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\tAccuracy: 0.85011\tPrecision: 0.28848\tRecall: 0.23800\tF1: 0.26082\tF2: 0.24663\n",
    "\tTotal predictions: 9000\tTrue positives:  238\tFalse positives:  587\tFalse negatives:  762\tTrue negatives: 7413\n",
    "\n",
    "random forest shared poi and loan advance e total payments. muito pior.\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\tAccuracy: 0.81386\tPrecision: 0.16776\tRecall: 0.07650\tF1: 0.10508\tF2: 0.08584\n",
    "\tTotal predictions: 14000\tTrue positives:  153\tFalse positives:  759\tFalse negatives: 1847\tTrue negatives: 11241\n",
    "\n",
    "Random com loan advance e salary muito pior que aqueles com features mais significativas.\n",
    "\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\tAccuracy: 0.70870\tPrecision: 0.19987\tRecall: 0.15200\tF1: 0.17268\tF2: 0.15965\n",
    "\tTotal predictions: 10000\tTrue positives:  304\tFalse positives: 1217\tFalse negatives: 1696\tTrue negatives: 6783\n",
    "\n",
    "Randon forest com 3 estimadores.\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=3, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\tAccuracy: 0.82720\tPrecision: 0.29919\tRecall: 0.22050\tF1: 0.25389\tF2: 0.23274\n",
    "\tTotal predictions: 15000\tTrue positives:  441\tFalse positives: 1033\tFalse negatives: 1559\tTrue negatives: 11967\n",
    "\n",
    "    Resultado melhor que com 10 porém pior que o naive bayes.\n",
    "    \n",
    "decision tree com todas as features \n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best')\n",
    "\tAccuracy: 0.80920\tPrecision: 0.27505\tRecall: 0.26350\tF1: 0.26915\tF2: 0.26573\n",
    "\tTotal predictions: 15000\tTrue positives:  527\tFalse positives: 1389\tFalse negatives: 1473\tTrue negatives: 11611\n",
    "\n",
    " \n",
    "Decision tree com  shared poi and loan advance. acima de 0.3 !!!!!\n",
    " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best')\n",
    "\tAccuracy: 0.85611\tPrecision: 0.34054\tRecall: 0.31500\tF1: 0.32727\tF2: 0.31980\n",
    "\tTotal predictions: 9000\tTrue positives:  315\tFalse positives:  610\tFalse negatives:  685\tTrue negatives: 7390\n",
    "\n",
    "decision tree com apenas shared poi\n",
    "\n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best')\n",
    "\tAccuracy: 0.84211\tPrecision: 0.30198\tRecall: 0.32100\tF1: 0.31120\tF2: 0.31701\n",
    "\tTotal predictions: 9000\tTrue positives:  321\tFalse positives:  742\tFalse negatives:  679\tTrue negatives: 7258\n",
    "  \n",
    "SVC, apraentemente classificadores lineares não são bons para reconher distribuições assimetricas. resultado muito ruim.\n",
    "Pipeline(memory=None,\n",
    "     steps=[('SelectKbest', SelectKBest(k=2, score_func=<function f_classif at 0x0000000009990588>)), ('rescalar', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False))])\n",
    "\tAccuracy: 0.86313\tPrecision: 0.13699\tRecall: 0.00500\tF1: 0.00965\tF2: 0.00619\n",
    "\tTotal predictions: 15000\tTrue positives:   10\tFalse positives:   63\tFalse negatives: 1990\tTrue negatives: 12937\n",
    "\n",
    "  \n",
    "  \n",
    "Utilizando pipeline com PCA(5 componentes) e decision tree pior que com 2 features.\n",
    "Pipeline(memory=None,\n",
    "     steps=[('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.80967\tPrecision: 0.26368\tRecall: 0.23850\tF1: 0.25046\tF2: 0.24314\n",
    "\tTotal predictions: 15000\tTrue positives:  477\tFalse positives: 1332\tFalse negatives: 1523\tTrue negatives: 11668\n",
    "\n",
    "Utilizando pipeline para decidir as melhores features gera disparidade em relação a escolha manual e reduz precision e recall\n",
    "Pipeline(memory=None,\n",
    "     steps=[('SelectKbest', SelectKBest(k=2, score_func=<function f_classif at 0x0000000009990588>)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.82933\tPrecision: 0.28528\tRecall: 0.18600\tF1: 0.22518\tF2: 0.19991\n",
    "\tTotal predictions: 15000\tTrue positives:  372\tFalse positives:  932\tFalse negatives: 1628\tTrue negatives: 12068\n",
    "\n",
    "\n",
    "usando pca para apenas duas features shared poi e loan advance. melhor até aqui !!!!!!\n",
    "\n",
    "Pipeline(memory=None,\n",
    "     steps=[('PCA', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.85822\tPrecision: 0.34902\tRecall: 0.31900\tF1: 0.33333\tF2: 0.32458\n",
    "\tTotal predictions: 9000\tTrue positives:  319\tFalse positives:  595\tFalse negatives:  681\tTrue negatives: 7405\n",
    "    \n",
    "    2 features porem 2 componentes de pca\n",
    "    Pipeline(memory=None,\n",
    "     steps=[('PCA', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.85833\tPrecision: 0.35038\tRecall: 0.32200\tF1: 0.33559\tF2: 0.32730\n",
    "\tTotal predictions: 9000\tTrue positives:  322\tFalse positives:  597\tFalse negatives:  678\tTrue negatives: 7403\n",
    "\n",
    "\n",
    "usando para as tres mehlores features aumentou bastante a precisão porém reduziu o recall, aumentou numero de falsos positivos\n",
    "peline(memory=None,\n",
    "     steps=[('PCA', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.80082\tPrecision: 0.43144\tRecall: 0.30050\tF1: 0.35426\tF2: 0.31992\n",
    "\tTotal predictions: 11000\tTrue positives:  601\tFalse positives:  792\tFalse negatives: 1399\tTrue negatives: 8208\n",
    "    \n",
    " rescalado, duas features alem de shared recepit loan advances\n",
    " \n",
    " Pipeline(memory=None,\n",
    "     steps=[('rescalar', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.82692\tPrecision: 0.40876\tRecall: 0.28000\tF1: 0.33234\tF2: 0.29883\n",
    "\tTotal predictions: 13000\tTrue positives:  560\tFalse positives:  810\tFalse negatives: 1440\tTrue negatives: 10190\n",
    "\n",
    "\n",
    "pca duas features alem de de shared recepit e lon advances\n",
    "\n",
    "Pipeline(memory=None,\n",
    "     steps=[('PCA', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.83938\tPrecision: 0.45985\tRecall: 0.25200\tF1: 0.32558\tF2: 0.27704\n",
    "\tTotal predictions: 13000\tTrue positives:  504\tFalse positives:  592\tFalse negatives: 1496\tTrue negatives: 10408\n",
    "    \n",
    "    \n",
    "    Estrategia com grid search\n",
    "    \n",
    "    2 criadas e loan e poi shared\n",
    "    GridSearchCV(cv=None, error_score='raise',\n",
    "       estimator=Pipeline(memory=None,\n",
    "     steps=[('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impu...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))]),\n",
    "       fit_params=None, iid=True, n_jobs=1,\n",
    "       param_grid={'reduce_dim__n_components': [1, 2, 3], 'clf__min_samples_split': [2, 4, 6, 8]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring='f1', verbose=0)\n",
    "\tAccuracy: 0.82808\tPrecision: 0.39897\tRecall: 0.23200\tF1: 0.29339\tF2: 0.25319\n",
    "\tTotal predictions: 13000\tTrue positives:  464\tFalse positives:  699\tFalse negatives: 1536\tTrue negatives: 10301\n",
    "{'reduce_dim__n_components': 3, 'clf__min_samples_split': 2}\n",
    "\n",
    "\n",
    "duas loan e shared poi scoring = F1\n",
    "GridSearchCV(cv=None, error_score='raise',\n",
    "       estimator=Pipeline(memory=None,\n",
    "     steps=[('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impu...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))]),\n",
    "       fit_params=None, iid=True, n_jobs=1,\n",
    "       param_grid={'reduce_dim__n_components': [1, 2], 'clf__min_samples_split': [2, 4, 6, 8]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring='f1', verbose=0)\n",
    "\tAccuracy: 0.84978\tPrecision: 0.29770\tRecall: 0.25900\tF1: 0.27701\tF2: 0.26591\n",
    "\tTotal predictions: 9000\tTrue positives:  259\tFalse positives:  611\tFalse negatives:  741\tTrue negatives: 7389\n",
    "\n",
    "{'reduce_dim__n_components': 2, 'clf__min_samples_split': 2}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
