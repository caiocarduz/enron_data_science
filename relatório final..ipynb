{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enron Submission Free-Response Questions\n",
    "\n",
    "\n",
    "A critical part of machine learning is making sense of your analysis process and communicating it to others. The questions below will help us understand your decision-making process and allow us to give feedback on your project. Please answer each question; your answers should be about 1-2 paragraphs per question. If you find yourself writing much more than that, take a step back and see if you can simplify your response!\n",
    "\n",
    "\n",
    "When your evaluator looks at your responses, he or she will use a specific list of rubric items to assess your answers. Here is the link to that rubric: [Link] Each question has one or more specific rubric items associated with it, so before you submit an answer, take a look at that part of the rubric. If your response does not meet expectations for all rubric points, you will be asked to revise and resubmit your project. Make sure that your responses are detailed enough that the evaluator will be able to understand the steps you took and your thought processes as you went through the data analysis.\n",
    "\n",
    "\n",
    "Once you’ve submitted your responses, your coach will take a look and may ask a few more focused follow-up questions on one or more of your answers.  \n",
    "\n",
    "\n",
    "We can’t wait to see what you’ve put together for this project!\n",
    "\n",
    "\n",
    "\n",
    "1-Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "## DataSet\n",
    "O dataset consiste em informações financeiras e de comunicação dos empregados da extinta corporação Enron, envolvida em um escândalo de fraudes contábeis que a levaram a falência e a intensa investigação por autoridades governamentais. \n",
    "De forma sucinta, a tentativa deste projeto é através destas informações identificar funcionários que participaram ativamente do esquema de fraude.\n",
    "As técnicas a serem utilizadas compreendem a exploração do dado, redução de dimensionalide, aplicação de um dos algoritmos de machine learnining e a validação dos resultados.\n",
    "\n",
    "## Exploração dos dados\n",
    "\n",
    "No total há 146 amostras e 20 \"features\". Muitos amostras apresentam features com \"null\" o que requeriu um preenchimento dessas ou por 0, ou alguma característica de tendência central, como média e mediana. Abaixo é possível ver todas as features e a quantidade de não null nas amostras.\n",
    "\n",
    "poi                          146 non-null bool\n",
    "\n",
    "bonus                        82 non-null float64\n",
    "\n",
    "deferred_income              49 non-null float64\n",
    "\n",
    "deferral_payments            39 non-null float64\n",
    "\n",
    "loan_advances                4 non-null float64\n",
    "\n",
    "other                        93 non-null float64\n",
    "\n",
    "expenses                     95 non-null float64\n",
    "\n",
    "director_fees                17 non-null float64\n",
    "\n",
    "total_payments               125 non-null float64\n",
    "\n",
    "exercised_stock_options      102 non-null float64\n",
    "\n",
    "restricted_stock             110 non-null float64\n",
    "\n",
    "restricted_stock_deferred    18 non-null float64\n",
    "\n",
    "total_stock_value            126 non-null float64\n",
    "\n",
    "to_messages                  86 non-null float64\n",
    "\n",
    "from_messages                86 non-null float64\n",
    "\n",
    "from_this_person_to_poi      86 non-null float64\n",
    "\n",
    "from_poi_to_this_person      86 non-null float64\n",
    "\n",
    "long_term_incentive          66 non-null float64\n",
    "\n",
    "salary                       95 non-null float64\n",
    "\n",
    "poi_email_ratio              86 non-null float64\n",
    "\n",
    "\n",
    "De forma a garantir a melhor qualidade das amostras tomou-se a estratégia de preencher as features financeiras sem valor com 0 pois se assumiu como premissa que valores nulos no dataset significam o não pagamento de determinado benefício ao funcionário.\n",
    "Abaixo é possível ver as caracteríticas das amostras posteriomente a substituição por 0.\n",
    "\n",
    "poi                          146 non-null bool\n",
    "\n",
    "bonus                        146 non-null float64\n",
    "\n",
    "deferred_income              146 non-null float64\n",
    "\n",
    "deferral_payments            146 non-null float64\n",
    "\n",
    "loan_advances                146 non-null float64\n",
    "\n",
    "other                        146 non-null float64\n",
    "\n",
    "expenses                     146 non-null float64\n",
    "\n",
    "director_fees                146 non-null float64\n",
    "\n",
    "total_payments               146 non-null float64\n",
    "\n",
    "exercised_stock_options      146 non-null float64\n",
    "\n",
    "restricted_stock             146 non-null float64\n",
    "\n",
    "restricted_stock_deferred    146 non-null float64\n",
    "\n",
    "total_stock_value            146 non-null float64\n",
    "\n",
    "to_messages                  86 non-null float64\n",
    "\n",
    "from_messages                86 non-null float64\n",
    "\n",
    "from_this_person_to_poi      86 non-null float64\n",
    "\n",
    "from_poi_to_this_person      86 non-null float64\n",
    "\n",
    "long_term_incentive          66 non-null float64\n",
    "\n",
    "salary                       95 non-null float64\n",
    "\n",
    "poi_email_ratio              146 non-null float64\n",
    "\n",
    "\n",
    "Com relação as variáveis de comunicação, optou-se pela substituição dos valores nulos pela média de cada uma das classes de POI. Neste caso, a falta de informação não a classifica necessariamente com a não interação entre os agentes e, portanto, o uso da mediana poderia ser justificado. Abaixo é mostrado as carateristicas gerais do dataset tratado.\n",
    "\n",
    "poi                          146 non-null bool\n",
    "\n",
    "bonus                        146 non-null float64\n",
    "\n",
    "deferred_income              146 non-null float64\n",
    "\n",
    "deferral_payments            146 non-null float64\n",
    "\n",
    "loan_advances                146 non-null float64\n",
    "\n",
    "other                        146 non-null float64\n",
    "\n",
    "expenses                     146 non-null float64\n",
    "\n",
    "director_fees                146 non-null float64\n",
    "\n",
    "total_payments               146 non-null float64\n",
    "\n",
    "exercised_stock_options      146 non-null float64\n",
    "\n",
    "restricted_stock             146 non-null float64\n",
    "\n",
    "restricted_stock_deferred    146 non-null float64\n",
    "\n",
    "total_stock_value            146 non-null float64\n",
    "\n",
    "to_messages                  146 non-null float64\n",
    "\n",
    "from_messages                146 non-null float64\n",
    "\n",
    "from_this_person_to_poi      146 non-null float64\n",
    "\n",
    "from_poi_to_this_person      146 non-null float64\n",
    "\n",
    "long_term_incentive          146 non-null float64\n",
    "\n",
    "salary                       146 non-null float64\n",
    "\n",
    "poi_to_email                 146 non-null float64\n",
    "\n",
    "poi_from_email               146 non-null float64\n",
    "\n",
    "\n",
    "Apenas um outlier foi encontrado o qual corresponde a amostra TOTAL. Esta soma todos os pagamentos efetuados pela empresa e por não corresponder a uma característica individual foi removida do datset.\n",
    "\n",
    "2-What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n",
    "\n",
    "\n",
    "## Seleção de features\n",
    "\n",
    "Para selecionar as principais features foram utilizados duas técnicas: KbestSelector, e DecisionTreeClassifier.\n",
    "\n",
    "O primeiro algoritmo classifica as features através de \"scores\" no qual aquelas que apresentarem valores maiores são as consideradas de maior relevância. De forma geral, os maiores \"scores\" foram atribuídos as variáveis financeiras e  classificação geral pode ser observado abaixo:\n",
    "\n",
    "[('exercised_stock_options', 24.95769208640269),\n",
    "\n",
    " ('total_stock_value', 24.554778199460614),\n",
    " \n",
    " ('bonus', 20.792252047181535),\n",
    " \n",
    " ('salary', 18.33836775749037),\n",
    " \n",
    " ('deferred_income', 11.466222228027991),\n",
    " \n",
    " ('long_term_incentive', 10.004464322324353),\n",
    " \n",
    " ('restricted_stock', 9.725202204751493),\n",
    " \n",
    " ('total_payments', 8.808881189792034),\n",
    " \n",
    " ('loan_advances', 7.184055658288725),\n",
    " \n",
    " ('expenses', 6.17412430056438),\n",
    " \n",
    " ('poi_from_email', 5.112000105553864),\n",
    " \n",
    " ('other', 4.369789663273286),\n",
    " \n",
    " ('from_poi_to_this_person', 2.962567365593336),\n",
    " \n",
    " ('poi_to_email', 2.10517496370243),\n",
    " \n",
    " ('director_fees', 1.9498838014353457),\n",
    " \n",
    " ('from_this_person_to_poi', 1.3200142577726062),\n",
    " \n",
    " ('from_messages', 0.6033909778782933),\n",
    " \n",
    " ('to_messages', 0.33780816490544013),\n",
    " \n",
    " ('deferral_payments', 0.2246112747360099),\n",
    " \n",
    " ('restricted_stock_deferred', 0.06549965290994214)]\n",
    " \n",
    " O outro algoritmo utilizado, diferentemente, não apresentou a mesma ordem de importância. A feature mais importante no caso da decision tree é bonus enquanto kbest é exercised stock option. a Abaixo é possível ver a relevância das features que tiverem \"score\" maiores que zero.\n",
    " \n",
    "other                        0.029966\n",
    "\n",
    "salary                       0.031684\n",
    "\n",
    "deferred_income              0.042245\n",
    "\n",
    "from_this_person_to_poi      0.049580\n",
    "\n",
    "from_messages                0.055447\n",
    "\n",
    "exercised_stock_options      0.058568\n",
    "\n",
    "expenses                     0.062307\n",
    "\n",
    "poi_to_email                 0.096561\n",
    "\n",
    "long_term_incentive          0.108631\n",
    "\n",
    "restricted_stock             0.120300\n",
    "\n",
    "total_payments               0.164592\n",
    "\n",
    "bonus                        0.180118\n",
    "\n",
    "Em termos de resultado, a segunda opção apresentou melhor resultado pois o classificador DecisionTree foi aquele que apresentou melhor precision e recall como veremos mais a frente. Posssivelmente, Kbest seria opção otimizada para algoritmos lineares como regressão logística.  \n",
    "\n",
    "## Scaling\n",
    "\n",
    "O \"scaling\" foi utilizado com os classificadores lineares como regressão logística e SVC porém quando comparados aos classificadores (decisionTree, Kneighbors e random forest) apresentaram resultado de f1 muito abaixo daqueles e portanto, não foram utilizados.\n",
    "\n",
    "## Criação de features\n",
    "\n",
    "foram criadas duas features, a saber: Poi_to_email e poi_from_email.\n",
    "A primeira é a razão entre emails enviados para pessoas de interesse e o total de mensagens recebidas. O racional desta feature e estabelecer com que frequencia, em relação ao total de mensagens enviadas, o agente se comumica com POIs o que poderia sugerir a sua qualificação como POIs ou não. \n",
    "No que tange a segunda feature, ela representa a razão entre emails recebidos de POIs e o total de emails recebidos. O racional é entender com que frequencia o agente recebe informações de POIs com relação a base total de contatos.\n",
    "As features foram criadas como funções.\n",
    "\n",
    "\n",
    "\n",
    "3-What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "\n",
    "O Algoritmo que apresentou melhor performance foi DecisionTree utilizando as features com relevância maior que 0. As features utilizadas, a saber:\n",
    "\n",
    "other                        0.029966\n",
    "\n",
    "salary                       0.031684\n",
    "\n",
    "deferred_income              0.042245\n",
    "\n",
    "from_this_person_to_poi      0.049580\n",
    "\n",
    "from_messages                0.055447\n",
    "\n",
    "exercised_stock_options      0.058568\n",
    "\n",
    "expenses                     0.062307\n",
    "\n",
    "poi_to_email                 0.096561\n",
    "\n",
    "long_term_incentive          0.108631\n",
    "\n",
    "restricted_stock             0.120300\n",
    "\n",
    "total_payments               0.164592\n",
    "\n",
    "bonus                        0.180118\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Os resultados relacionados a melhor performance obtidos, a saber:\n",
    "\n",
    "Pipeline(memory=None,\n",
    "\n",
    "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "     \n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight={0: 6, 1: 10}, criterion='gini',\n",
    "  \n",
    "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "            \n",
    "            min_impur...        min_weight_fraction_leaf=0.0, presort=False, random_state=29,\n",
    "            \n",
    "            splitter='best'))])\n",
    "            \n",
    "        Accuracy: 0.84713       Precision: 0.42398      Recall: 0.40850 F1: 0.41609     F2: 0.41150\n",
    "        \n",
    "        Total predictions: 15000        True positives:  817    False positives: 1110   False negatives: 1183   True negatives: 11890\n",
    "\n",
    "Diversos classificadores foram utilizados porém a performance não foram satisfatórias. As features selecionadas nestes casos foram aqueles determinadas pelo KbestSelector. Duas estratégias foram seguidas. O uso de PCAs e sem o uso delas. O resumo das rodadas podem ser vistos abaixo.\n",
    "\n",
    "Pipeline(memory=None,\n",
    "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            ...n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False))])\n",
    "            \n",
    "        Accuracy: 0.85067       Precision: 0.39619      Recall: 0.22900 F1: 0.29024     F2: 0.25011\n",
    "        \n",
    "        Total predictions: 15000        True positives:  458    False positives:  698   False negatives: 1542   True negatives: 12302\n",
    "\n",
    "    \n",
    "Pipeline(memory=None,\n",
    "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', GaussianNB(priors=None))])\n",
    "  \n",
    "        Accuracy: 0.87280       Precision: 0.54457      Recall: 0.28100 F1: 0.37071     F2: 0.31112\n",
    "        \n",
    "        Total predictions: 15000        True positives:  562    False positives:  470   False negatives: 1438   True negatives: 12530\n",
    "\n",
    "    \n",
    "Pipeline(memory=None,\n",
    "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('selector', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
    "          \n",
    "        Accuracy: 0.88067       Precision: 0.66509      Recall: 0.21150 F1: 0.32094     F2: 0.24491\n",
    "        \n",
    "        Total predictions: 15000        True positives:  423    False positives:  213   False negatives: 1577   True negatives: 12787\n",
    "     \n",
    "Pipeline(memory=None,\n",
    "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
    "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
    "    random_state=None, tol=0.0001, verbose=0))])\n",
    "    \n",
    "        Accuracy: 0.84207       Precision: 0.25168      Recall: 0.09350 F1: 0.13635     F2: 0.10694\n",
    "        \n",
    "        Total predictions: 15000        True positives:  187    False positives:  556   False negatives: 1813   True negatives: 12444\n",
    "        \n",
    "        \n",
    "Estratégia sem pca, somente as 5 features mais importantes.\n",
    "\n",
    "Pipeline(memory=None,\n",
    "     steps=[('selector', SelectKBest(k=5, score_func=<function f_classif at 0x000000000C790AC8>)), ('clf', GaussianNB(priors=None))])\n",
    "     \n",
    "        Accuracy: 0.85340       Precision: 0.43699      Recall: 0.34500 F1: 0.38558     F2: 0.36016\n",
    "        \n",
    "        Total predictions: 15000        True positives:  690    False positives:  889   False negatives: 1310   True negatives: 12111\n",
    "        \n",
    "Pipeline(memory=None,\n",
    "     steps=[('selector', SelectKBest(k=5, score_func=<function f_classif at 0x000000000C790AC8>)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_...n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False))])\n",
    "            \n",
    "        Accuracy: 0.86273       Precision: 0.46674      Recall: 0.20700 F1: 0.28680     F2: 0.23292\n",
    "        \n",
    "        Total predictions: 15000        True positives:  414    False positives:  473   False negatives: 1586   True negatives: 12527 \n",
    "\n",
    "Pipeline(memory=None,\n",
    "     steps=[('selector', SelectKBest(k=5, score_func=<function f_classif at 0x000000000C790AC8>)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
    "          \n",
    "        Accuracy: 0.86153       Precision: 0.45019      Recall: 0.17400 F1: 0.25099     F2: 0.19834\n",
    "        \n",
    "        Total predictions: 15000        True positives:  348    False positives:  425   False negatives: 1652   True negatives: 12575\n",
    "\n",
    "Pipeline(memory=None,\n",
    "     steps=[('selector', SelectKBest(k=5, score_func=<function f_classif at 0x000000000C790AC8>)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
    "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
    "    random_state=None, tol=0.0001, verbose=0))])\n",
    "    \n",
    "        Accuracy: 0.62433       Precision: 0.14495      Recall: 0.37100 F1: 0.20846     F2: 0.28280\n",
    "        \n",
    "        Total predictions: 15000        True positives:  742    False positives: 4377   False negatives: 1258   True negatives: 8623\n",
    "\n",
    "A métrica de maior relevância para se caracterizar o melhor classificador é o F1 a qual é representada pela média harmônica da precisão e recall. Lembrar que o intuito do é minimizar o número de falsos positivos e falsos negativos de forma equilibrada e não um as custas do outro. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4-What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "\n",
    "\"tuning\" é essencialmente optimizar os parametros do classificador selecionado de forma que permita que o mesmo apresente a melhor performance possível em termos do trade-off Viés-Variância. Entenda-se por performance, maximizar acurácia, precision e recall do modelo.\n",
    "No projeto foi feito escolha manual por tentiva erro. Porém, existe fluxos mais otimizados como o GridSearhCV que permite que diversos parametros sejam testados e escolhidos automaticamente de acordo com a melhor performance em termos de métrica.\n",
    "No caso em questão, os parâmetros do mehlor classificador foi garantir um peso maior as amostras positivas devido a assimetria da distribuição dos eventos. Além disso, a diminuição da dimensionalidade do problema através do uso de uma PCA com duas componentes permitiu um melhor F1.\n",
    "\n",
    "Pipeline(memory=None,\n",
    "\n",
    "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "     \n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight={0: 6, 1: 10}, criterion='gini',\n",
    "  \n",
    "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "            \n",
    "            min_impur...        min_weight_fraction_leaf=0.0, presort=False, random_state=29,\n",
    "            \n",
    "            splitter='best'))])\n",
    "            \n",
    "        Accuracy: 0.84713       Precision: 0.42398      Recall: 0.40850 F1: 0.41609     F2: 0.41150\n",
    "        \n",
    "        Total predictions: 15000        True positives:  817    False positives: 1110   False negatives: 1183   True negatives: 11890\n",
    "\n",
    "\n",
    "5-What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]\n",
    "\n",
    "Validação é medir a performance do classificador. Erro mais comum é treinar seu modelo e prevê-lo no mesmo conjunto de dados. ou seja, nao separa-lo em training e test data. Este tipo de erro pode levar as situação de \"overfitting\" que é a incapacidade de generalizar.\n",
    "Há diversas formas de validação, neste projeto foi utilizado o StratifiedShuffleSplit, que é essencialmente um  método de validação cruzada cuja característica é separar o dado em training/test, treinar/prever o resultado diversas vezes com pares training/test diferentes e, ao final, produzir a média dos resultados. Uma característica deste método é a sobreposição dos conjuntos selecionados para a validação cruzada o que não acontece com o método Kfolds. \n",
    "\n",
    "6-Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "Os resultados relacionados a performance obtidos, a saber:\n",
    "\n",
    "ccuracy: 0.84713       Precision: 0.42398      Recall: 0.40850 F1: 0.41609     F2: 0.41150\n",
    "\n",
    "Precisão refere-se a razão de verdadeiro positivo e a soma dos falsos positivos e verdadeiros positivos. Ou seja, da a proporção, dentre aqueles que foram previstos como POIs, quantos efetivamente são POIs. De outra forma, quando o modelo preve que é POIs, ele está certo em 42,4% dos casos.\n",
    "\n",
    "Recall refere-se a razão veradeiro positivo e a soma do verdadeiro positivo e falso negativo. Ou seja, da a proporção de quantos POIs foram identificados corretamente. De outra forma, o modelo indentifica corretamente 41,6% dos POIs.\n",
    "\n",
    "Já o F1 é a média harmônica entre precision e recall. quanto mais próxima de 1 melhor a capcidade de previsão do modelo.\n",
    "\n",
    "## Referências \n",
    "\n",
    "https://olegleyz.github.io/enron_classifier.html\n",
    "\n",
    "https://github.com/nehal96/Machine-Learning-Enron-Fraud\n",
    "\n",
    "http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "https://pandas.pydata.org/\n",
    "\n",
    "https://www.tutorialspoint.com/python/python_dictionary.htm\n",
    "\n",
    "https://pt.wikipedia.org/wiki/Enron\n",
    "\n",
    "http://bailando.sims.berkeley.edu/enron_email.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memorial de cálculo.\n",
    "\n",
    "\n",
    "Configuração inicial, somente a feature Salary.\n",
    "\n",
    "GaussianNB(priors=None)\n",
    "\tAccuracy: 0.25560\tPrecision: 0.18481\tRecall: 0.79800\tF1: 0.30011\tF2: 0.47968\n",
    "\tTotal predictions: 10000\tTrue positives: 1596\tFalse positives: 7040\tFalse negatives:  404\tTrue negatives:  960\n",
    "\n",
    "\n",
    "Config inicial porem com rescolanamento de todas as features\n",
    "    GaussianNB(priors=None)\n",
    "\tAccuracy: 0.33800\tPrecision: 0.15456\tRecall: 0.88700\tF1: 0.26324\tF2: 0.45539\n",
    "\tTotal predictions: 15000\tTrue positives: 1774\tFalse positives: 9704\tFalse negatives:  226\tTrue negatives: 3296\n",
    "\n",
    "Config com 2 features shared poi e loan advance \n",
    "    GaussianNB(priors=None)\n",
    "\tAccuracy: 0.33585\tPrecision: 0.16857\tRecall: 0.84350\tF1: 0.28098\tF2: 0.46840\n",
    "\tTotal predictions: 13000\tTrue positives: 1687\tFalse positives: 8321\tFalse negatives:  313\tTrue negatives: 2679\n",
    "\n",
    "\n",
    "Randon forest com 10 estimadores\n",
    "\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\tAccuracy: 0.84940\tPrecision: 0.30408\tRecall: 0.10050\tF1: 0.15107\tF2: 0.11604\n",
    "\tTotal predictions: 15000\tTrue positives:  201\tFalse positives:  460\tFalse negatives: 1799\tTrue negatives: 12540\n",
    "    \n",
    "    Notar maior acuracia, no entanto menor precisão e recall levando F1 ser a metade do naive bayes. \n",
    "    Provavelmente o grande numero de features tenha sido o grande vilão. Re-escalar os dados pode ser uma alternativa, ou usar um numero menor de estimadores.\n",
    "    \n",
    "Rabdom forest com um estimador shared poi\n",
    "    andomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\tAccuracy: 0.83767\tPrecision: 0.26741\tRecall: 0.26500\tF1: 0.26620\tF2: 0.26548\n",
    "\tTotal predictions: 9000\tTrue positives:  265\tFalse positives:  726\tFalse negatives:  735\tTrue negatives: 7274\n",
    "    \n",
    "random forest shared poi and loan advance\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\tAccuracy: 0.85011\tPrecision: 0.28848\tRecall: 0.23800\tF1: 0.26082\tF2: 0.24663\n",
    "\tTotal predictions: 9000\tTrue positives:  238\tFalse positives:  587\tFalse negatives:  762\tTrue negatives: 7413\n",
    "\n",
    "random forest shared poi and loan advance e total payments. muito pior.\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\tAccuracy: 0.81386\tPrecision: 0.16776\tRecall: 0.07650\tF1: 0.10508\tF2: 0.08584\n",
    "\tTotal predictions: 14000\tTrue positives:  153\tFalse positives:  759\tFalse negatives: 1847\tTrue negatives: 11241\n",
    "\n",
    "Random com loan advance e salary muito pior que aqueles com features mais significativas.\n",
    "\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\tAccuracy: 0.70870\tPrecision: 0.19987\tRecall: 0.15200\tF1: 0.17268\tF2: 0.15965\n",
    "\tTotal predictions: 10000\tTrue positives:  304\tFalse positives: 1217\tFalse negatives: 1696\tTrue negatives: 6783\n",
    "\n",
    "Randon forest com 3 estimadores.\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=3, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\tAccuracy: 0.82720\tPrecision: 0.29919\tRecall: 0.22050\tF1: 0.25389\tF2: 0.23274\n",
    "\tTotal predictions: 15000\tTrue positives:  441\tFalse positives: 1033\tFalse negatives: 1559\tTrue negatives: 11967\n",
    "\n",
    "    Resultado melhor que com 10 porém pior que o naive bayes.\n",
    "    \n",
    "decision tree com todas as features \n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best')\n",
    "\tAccuracy: 0.80920\tPrecision: 0.27505\tRecall: 0.26350\tF1: 0.26915\tF2: 0.26573\n",
    "\tTotal predictions: 15000\tTrue positives:  527\tFalse positives: 1389\tFalse negatives: 1473\tTrue negatives: 11611\n",
    "\n",
    " \n",
    "Decision tree com  shared poi and loan advance. acima de 0.3 !!!!!\n",
    " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best')\n",
    "\tAccuracy: 0.85611\tPrecision: 0.34054\tRecall: 0.31500\tF1: 0.32727\tF2: 0.31980\n",
    "\tTotal predictions: 9000\tTrue positives:  315\tFalse positives:  610\tFalse negatives:  685\tTrue negatives: 7390\n",
    "\n",
    "decision tree com apenas shared poi\n",
    "\n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best')\n",
    "\tAccuracy: 0.84211\tPrecision: 0.30198\tRecall: 0.32100\tF1: 0.31120\tF2: 0.31701\n",
    "\tTotal predictions: 9000\tTrue positives:  321\tFalse positives:  742\tFalse negatives:  679\tTrue negatives: 7258\n",
    "  \n",
    "SVC, apraentemente classificadores lineares não são bons para reconher distribuições assimetricas. resultado muito ruim.\n",
    "Pipeline(memory=None,\n",
    "     steps=[('SelectKbest', SelectKBest(k=2, score_func=<function f_classif at 0x0000000009990588>)), ('rescalar', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False))])\n",
    "\tAccuracy: 0.86313\tPrecision: 0.13699\tRecall: 0.00500\tF1: 0.00965\tF2: 0.00619\n",
    "\tTotal predictions: 15000\tTrue positives:   10\tFalse positives:   63\tFalse negatives: 1990\tTrue negatives: 12937\n",
    "\n",
    "  \n",
    "  \n",
    "Utilizando pipeline com PCA(5 componentes) e decision tree pior que com 2 features.\n",
    "Pipeline(memory=None,\n",
    "     steps=[('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.80967\tPrecision: 0.26368\tRecall: 0.23850\tF1: 0.25046\tF2: 0.24314\n",
    "\tTotal predictions: 15000\tTrue positives:  477\tFalse positives: 1332\tFalse negatives: 1523\tTrue negatives: 11668\n",
    "\n",
    "Utilizando pipeline para decidir as melhores features gera disparidade em relação a escolha manual e reduz precision e recall\n",
    "Pipeline(memory=None,\n",
    "     steps=[('SelectKbest', SelectKBest(k=2, score_func=<function f_classif at 0x0000000009990588>)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.82933\tPrecision: 0.28528\tRecall: 0.18600\tF1: 0.22518\tF2: 0.19991\n",
    "\tTotal predictions: 15000\tTrue positives:  372\tFalse positives:  932\tFalse negatives: 1628\tTrue negatives: 12068\n",
    "\n",
    "\n",
    "usando pca para apenas duas features shared poi e loan advance. melhor até aqui !!!!!!\n",
    "\n",
    "Pipeline(memory=None,\n",
    "     steps=[('PCA', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.85822\tPrecision: 0.34902\tRecall: 0.31900\tF1: 0.33333\tF2: 0.32458\n",
    "\tTotal predictions: 9000\tTrue positives:  319\tFalse positives:  595\tFalse negatives:  681\tTrue negatives: 7405\n",
    "    \n",
    "    2 features porem 2 componentes de pca\n",
    "    Pipeline(memory=None,\n",
    "     steps=[('PCA', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.85833\tPrecision: 0.35038\tRecall: 0.32200\tF1: 0.33559\tF2: 0.32730\n",
    "\tTotal predictions: 9000\tTrue positives:  322\tFalse positives:  597\tFalse negatives:  678\tTrue negatives: 7403\n",
    "\n",
    "\n",
    "usando para as tres mehlores features aumentou bastante a precisão porém reduziu o recall, aumentou numero de falsos positivos\n",
    "peline(memory=None,\n",
    "     steps=[('PCA', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.80082\tPrecision: 0.43144\tRecall: 0.30050\tF1: 0.35426\tF2: 0.31992\n",
    "\tTotal predictions: 11000\tTrue positives:  601\tFalse positives:  792\tFalse negatives: 1399\tTrue negatives: 8208\n",
    "    \n",
    " rescalado, duas features alem de shared recepit loan advances\n",
    " \n",
    " Pipeline(memory=None,\n",
    "     steps=[('rescalar', MinMaxScaler(copy=True, feature_range=(0, 1))), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.82692\tPrecision: 0.40876\tRecall: 0.28000\tF1: 0.33234\tF2: 0.29883\n",
    "\tTotal predictions: 13000\tTrue positives:  560\tFalse positives:  810\tFalse negatives: 1440\tTrue negatives: 10190\n",
    "\n",
    "\n",
    "pca duas features alem de de shared recepit e lon advances\n",
    "\n",
    "Pipeline(memory=None,\n",
    "     steps=[('PCA', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.83938\tPrecision: 0.45985\tRecall: 0.25200\tF1: 0.32558\tF2: 0.27704\n",
    "\tTotal predictions: 13000\tTrue positives:  504\tFalse positives:  592\tFalse negatives: 1496\tTrue negatives: 10408\n",
    "    \n",
    "    \n",
    "    Estrategia com grid search\n",
    "    \n",
    "    2 criadas e loan e poi shared\n",
    "    GridSearchCV(cv=None, error_score='raise',\n",
    "       estimator=Pipeline(memory=None,\n",
    "     steps=[('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impu...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))]),\n",
    "       fit_params=None, iid=True, n_jobs=1,\n",
    "       param_grid={'reduce_dim__n_components': [1, 2, 3], 'clf__min_samples_split': [2, 4, 6, 8]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring='f1', verbose=0)\n",
    "\tAccuracy: 0.82808\tPrecision: 0.39897\tRecall: 0.23200\tF1: 0.29339\tF2: 0.25319\n",
    "\tTotal predictions: 13000\tTrue positives:  464\tFalse positives:  699\tFalse negatives: 1536\tTrue negatives: 10301\n",
    "{'reduce_dim__n_components': 3, 'clf__min_samples_split': 2}\n",
    "\n",
    "\n",
    "duas loan e shared poi scoring = F1\n",
    "GridSearchCV(cv=None, error_score='raise',\n",
    "       estimator=Pipeline(memory=None,\n",
    "     steps=[('reduce_dim', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impu...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))]),\n",
    "       fit_params=None, iid=True, n_jobs=1,\n",
    "       param_grid={'reduce_dim__n_components': [1, 2], 'clf__min_samples_split': [2, 4, 6, 8]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring='f1', verbose=0)\n",
    "\tAccuracy: 0.84978\tPrecision: 0.29770\tRecall: 0.25900\tF1: 0.27701\tF2: 0.26591\n",
    "\tTotal predictions: 9000\tTrue positives:  259\tFalse positives:  611\tFalse negatives:  741\tTrue negatives: 7389\n",
    "\n",
    "{'reduce_dim__n_components': 2, 'clf__min_samples_split': 2}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
